{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide avanzate OpenAI assistants\n",
    "\n",
    "## Introduzione\n",
    "\n",
    "Questo notebook servirà ad introdurre e a spiegare alcuni elementi più avanzati sugli strumenti messi a disposizione da OpenAI. E' raccomandata la lettura dei notebooks \"RAG\" e \"OpenAI Assistants Base\" prima di questo.    \n",
    "\n",
    "Ci saranno diverse sezioni (perlopiù indipendenti una dall'altra) in cui verranno introdotti argomenti diversi.\n",
    "\n",
    "Gli argomenti trattati saranno:\n",
    "\n",
    "- Streaming: come mostrare all'utente in tempo reale le parole mentre vengono generate dall'Assistant\n",
    "- File Annotations: come utilizzare i riferimenti che compaiono all'interno delle risposte di un'Assistant, e come farle puntare al documento.\n",
    "- Function Calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Nel notebook base abbiamo visto come ottenere le risposte in modalità sincrona: questo significa che noi (e l'utente finale) potremo vedere la risposta soltanto quando essa ha terminato di essere \"assemblata\" dal modello.  \n",
    "  \n",
    "Con lo streming invece il testo fluisce in chunks direttamente mentre il modello lo crea. Con l'SDK base di OpenAI lo streaming viaggia sotto forma di chunks che arrivano seguendo lo standard SSE (Server Sent Events). Noi possiamo scegliere di accumulare questi chunks fino a quando arrierà l'ultimo, che chiameremo stop-chunk.\n",
    "\n",
    "Penso sia utile vedere in apertura come funziona lo streaming nell'SDK classico di OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La teoria del multiverso è una ipotesi fisica che suggerisce l'esistenza di un numero infinito di universi diversi dal nostro, ognuno con le proprie leggi fisiche e costanti fondamentali. Secondo questa teoria, ogni possibile universo che potrebbe esistere, esiste effettivamente in un certo punto del multiverso.\n",
      "\n",
      "Questa ipotesi è stata proposta per spiegare alcune delle inconsistenze e delle apparenti contraddizioni nella fisica moderna, come ad esempio il concetto di universo inflazionario, il paradosso di Schrödinger e il problema dei costanti fondamentali. La teoria del multiverso suggerisce che, anziché cercare una spiegazione unificata per tutti questi fenomeni, potrebbero esistere molteplici universi che si comportano in modi diversi.\n",
      "\n",
      "È importante sottolineare che la teoria del multiverso è ancora oggetto di dibattito tra gli scienziati e non è attualmente provata sperimentalmente. Tuttavia, questa ipotesi offre un'interessante prospettiva sulla natura dell'universo e ha stimolato molte ricerche e teorie nella fisica moderna."
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Spiegami la teoria del multiverso\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere che ha impiegato un certo tempo, ma comunque i chunks sono spuntati correttamente.  \n",
    "Lo streaming inizia sempre con un chunk vuoto, e finisce sempre con uno stop chunk, riconoscibile per la finish_reason (chunk.choices[0].finish_reason) non nulla (solitamente \"stop\")  \n",
    "La struttura di un singolo chunk è:\n",
    "```json\n",
    "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0125\", \"system_fingerprint\": \"fp_44709d6fcb\", \"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\n",
    "\n",
    "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0125\", \"system_fingerprint\": \"fp_44709d6fcb\", \"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"logprobs\":null,\"finish_reason\":null}]}\n",
    "\n",
    "....\n",
    "\n",
    "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-3.5-turbo-0125\", \"system_fingerprint\": \"fp_44709d6fcb\", \"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sede di produzione, basta stabilire una connessione asincrona con il frontend (WebSocket solitamente) e effettuare il mirroring dei chunks man mano che arrivano, chiudendo il messaggio quando arriva lo stop-chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming con OpenAI Assistants\n",
    "\n",
    "Quando lavoriamo con gli assistants di OpenAI abbiamo sempre la possibilità di utilizzare lo streaming, anche in ragione del fatto che il tempo di generazione per una risposta è solitamente maggiore rispetto ad una chiamata classica ai modelli.  \n",
    "\n",
    "In questo caso, lo streaming arriverà attraverso l'oggetto che istanziamo quando eseguiamo il RUN sul thread. Useremo (come al solito) l'assistant con la documentazione di IVIC.  \n",
    "\n",
    "Iniziamo: creiamo un thread e inseriamo un messaggio user con una domanda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_WNadRumwkmStfH5L90yGFNwt', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Cosa si intende per domanda di disambiguazione?'), type='text')], created_at=1718095285, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_e1Jb6NUjJ56Rg4RrROs8Ngud')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IVIC_ASST = \"asst_RUodZv7nz1qutEcGB6a1z8Do\"\n",
    "ivic_thread = client.beta.threads.create()\n",
    "client.beta.threads.messages.create(\n",
    "                thread_id=ivic_thread.id,                          \n",
    "                role=\"user\",                                        \n",
    "                content=\"Cosa si intende per domanda di disambiguazione?\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per funzionare, un run con streaming ha bisogno di una classe EventHandler, che specifica le azioni che il nostro runtime dovrà eseguire quando si verificano determinati eventi nel flusso di streaming che ci arriva.\n",
    "\n",
    "Vediamo ora un EventHandler base, che si limita a dire quando viene effettuata la ricerca nel vectorstore, e mi fa lo streaming del solo testo.  \n",
    "\n",
    "Quua sotto possiamo vedere il funzionamento di un semplice eventHandler base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importo il decoratore che mi servità per sovrascrivere i metodi già presenti in runtime\n",
    "from typing_extensions import override  \n",
    "\n",
    "# importo la superclasse di event handler\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "class EventHandler(AssistantEventHandler): \n",
    "  \n",
    "  # metodo invocato quando il messaggio inizia, e che aggiunge una piccola intestazione al messaggio (\"Ivic_assistente >\").\n",
    "  # se volessi fare operazioni sul contenuto del messaggio nascituro, devo azionarle sulla variabile locale \"text\"\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None: \n",
    "    print(f\"\\nIvic_assistente > \", end=\"\", flush=True)\n",
    "    \n",
    "  # metodo invocato ad ogni nuovo chunk\n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "    \n",
    "  # metodo invocato quando un tool (file_search o code_intepreter) esegue un'azione\n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nEvento > {tool_call.type}\\n\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evento > file_search\n",
      "\n",
      "\n",
      "Ivic_assistente > Una domanda di disambiguazione in un software come ivic è una richiesta posta all'utente per chiarire e descrivere brevemente una transazione che potrebbe non essere stata identificata in modo chiaro come parte di un determinato schema di frode. Se le regole di identificazione non sono state sufficienti a individuare con sicurezza un pattern di frode specifico, viene posta una domanda di disambiguazione per ottenere maggiori dettagli sulla transazione dall'utente. La risposta dell'utente viene poi analizzata tramite un algoritmo di intelligenza artificiale che incrocia le informazioni fornite dall'utente con le descrizioni ufficiali dei modelli di frode per determinare se la transazione potrebbe rientrare in un determinato schema. Basandosi su questa analisi, il software può produrre diversi esiti, tra cui la declinazione della transazione o la conferma di validità della transazione【4:0†source】【4:2†source】."
     ]
    }
   ],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=ivic_thread.id,\n",
    "  assistant_id=IVIC_ASST,\n",
    " event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".chenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
